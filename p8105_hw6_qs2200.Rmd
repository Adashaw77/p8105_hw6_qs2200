---
title: "P8105_hw6_qs2200"
author: "Qi Shao"
date: "11/20/2018"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(purrr)
library(modelr)
library(mgcv)
```

##Problem 1

Tidy the data.
```{r problem 1.1, message= F}
homi_df = read_csv("./data/homicide-data.csv") 
homi_df = 
  homi_df%>%
  mutate(city_state = str_c(city, ", ", state)) %>%
  janitor::clean_names() %>%
  mutate(homi_solved= ifelse(disposition == "Closed by arrest", 1, 0)) %>%
  filter(!(city_state %in%c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))) %>%
  mutate(victim_race = ifelse(victim_race == "White", "white", "non-white"), victim_age = as.integer(victim_age), victim_race = fct_relevel(victim_race, "white", "non-white")) 
```

```{r problem 1.2}
balti_model = homi_df %>%
  filter(city_state == "Baltimore, MD")

glm(homi_solved ~ victim_age + victim_sex + victim_race, data = balti_model, family = binomial()) %>%
broom::tidy() %>%
mutate(OR_estimate = exp(estimate), OR_conf_low = exp(estimate-qnorm(.975)*std.error), OR_conf_high = exp(estimate+qnorm(.975)*std.error)) %>%
filter(term == "victim_racenon-white") %>%
dplyr::select(term,OR_conf_low, OR_estimate, OR_conf_high) %>% 
knitr::kable(digits = 3)
```

The result of linear model shows that the adjusted odds ratio of solved homicides in which the victim is non-white estimated to be 0.441 times , with 95% confidence interval from 0.313 to 0.62, the odds ratio of homicides in which the victim is white.

```{r problem 1.3, fig.height = 7}

each_city = homi_df %>%
  group_by(city_state) %>%
  nest()

odds_glm = function(df){
  glm( homi_solved ~ victim_age + victim_sex + victim_race, data = df, family =     binomial())  %>%
  broom::tidy() %>%
  mutate(OR_estimate = exp(estimate), OR_conf_low =        exp(estimate-qnorm(.975)*std.error), OR_conf_high = exp(estimate+qnorm(.975)*std.error)) %>%
  filter(term == "victim_racenon-white") %>%
  dplyr::select(term,OR_conf_low, OR_estimate, OR_conf_high)
}

all_estimate = 
  mutate(each_city, or = map(data, odds_glm)) %>%
  dplyr::select(city_state, or) %>%
  unnest() %>%
  dplyr::select(-term)

all_estimate %>%
  mutate(city_state = fct_reorder(city_state, OR_estimate)) %>%
  ggplot(aes(x = city_state,y = OR_estimate)) + 
  geom_point() +
  geom_errorbar(aes(ymin = OR_conf_low,ymax = OR_conf_high))+
  geom_hline(yintercept = 1, alpha = 0.5, color = "red")+
  coord_flip()+
  theme_minimal()+
  labs(title = "Estimate and 95% CI of solving homicides odds ratio for each city", 
         x = "City and State", 
         y = "Solving homicides odds ratio comparing non-white victims to white victims") 
```

From the plot, we can see in most cities in the study, the estimate odds ratio of solving homicides comparing non-white victims to white victims is less than 1. It indicate that in most of the cities, homicides in which the victim is non-white are substantially less likely to be resolved that those in which the victim is white. There are also some cities in which CI of odds ratio including 1, and 3 of them have estimate odds ratio greater than 1. In these cities, homicides in which the victim is non-white are more likely to be resolved that those in which the victim is white.

## Problem 2
### Tidy the data
```{r problem 2.1}
birthweight_df = read.csv("./data/birthweight.csv") 

birthweight_df = janitor::clean_names(birthweight_df) %>%
  dplyr::mutate(babysex = as.factor(as.character(babysex)), 
                frace = as_factor(as.character(frace)),
                mrace = as_factor(as.character(mrace)),
                malform = as_factor(as.character(malform))) %>%
  dplyr::select(bwt, everything())

skimr::skim(birthweight_df) 
```

There are 4342 observations and 10 variables in this dataset. 4 factor variables are babysex, father race, mother race and presence of malformations. There are no missing value in this dataset.

### Propose a regression model

*Check the distribution of outcome*

```{r}
birthweight_df %>%
  ggplot(aes(x = bwt)) + 
  geom_density() +
  labs(title = "Distribution of birthweight variable")
```
We can see that the birth weight is approximately distributed normally, satisfying linear regression model assumption.

*Multicolinearity*

```{r, eval = F}
pairs(birthweight_df)
```
We could visualize the pairwise distribution of variables. And we can see the relationship between bwt and bhead/blength are in linear form.

There are also some pair of predictors shows linear relationship: bhead and blength, delwt and ppbmi, delwt and ppwt, ppbmi and ppwt. We should concern the multicolinearity of these predictors.

*Use backward elimination method to build a regression model, take out non-significant variables one at a time.*
```{r problem 2.3, results="hide"}
mult.fit = lm(bwt ~ ., data = birthweight_df)
step(mult.fit, direction='backward')
```

```{r}
fit_model1 = lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthweight_df)
summary(fit_model1)
```
The result of backward stepwise selection have 11 predictors.

*Diagnose*
```{r}
par(mfrow = c(2,2))
plot(fit_model1)
```
We can see the model fits well.

```{r}
birthweight_df %>%
  modelr::add_residuals(fit_model1) %>% 
  modelr::add_predictions(fit_model1) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = .4)+
  geom_hline(yintercept = 0, color = "red") +
  ggtitle("Residuals vs Fitted value")
```

The plot indicates that for the fitted values bewteen 2000 and 4000, the residuals evenly distributed around zero. It means the model fits well when fitted value are between 2000 and 4000.

```{r}
fit_model2 = lm(bwt ~ blength + gaweeks, data = birthweight_df)
summary(fit_model2)

fit_model3 = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = birthweight_df)
summary(fit_model3)

set.seed(1)
cv_df = 
  crossv_mc(birthweight_df, 100)%>%
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble)) %>%
  mutate(fit_model1 = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
         fit_model2 = map(train, ~lm(bwt ~ blength + gaweeks,data = .x)),
         fit_model3 = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))) %>% 
  mutate(rmse_model1 = map2_dbl(fit_model1, test, ~rmse(model = .x, data = .y)),
         rmse_model2 = map2_dbl(fit_model2, test, ~rmse(model = .x, data = .y)),
         rmse_model3 = map2_dbl(fit_model3, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  dplyr::select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

From the plot above, we found that model1, which is created by backward elimination method, has the lowest RMSE and model_2 has the highest. We CAN conclude that model1 fits the data better than model2 and 3.